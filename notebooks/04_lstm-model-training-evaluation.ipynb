{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8565052,"sourceType":"datasetVersion","datasetId":5119940},{"sourceId":8565606,"sourceType":"datasetVersion","datasetId":5120786}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Module 4 - LSTM Model*\n___","metadata":{}},{"cell_type":"markdown","source":"# 1. Project summary\n___\n\nThis is part of a project to build a sentiment classifier trained on Yelp review data (https://www.yelp.com/dataset). The project has been divided into several modules to perform different parts of the analysis, e.g., data cleaning, data processing, and model training. The goal is to predict the sentiment of a document; while using Yelp reviews of businesses, the 1-5 star rating acts as a proxy for sentiment, and the written Yelp review as the document text. The project is written in Python on Jupyter notebooks and makes use of a range of data science tools like pandas, spaCy, word2vec, and keras. My motivation in starting this project is to build my skillset, learn new tools, and improve as a data scientist. It is an ongoing project and may see many updates/iterations.","metadata":{}},{"cell_type":"markdown","source":"# 2. Module overview\n___\n\n## Goal\n- The goal of this module is to train a classifier to identify the sentiment of a text document using Yelp review data\n    - Data Splitting:\n        - Split data into training and testing samples\n    - Modeling:\n        - Define the structure of the model using LSTM\n    - Training:\n        - Train the model using the encoded Yelp data\n    - Evaluation\n        - Evvaluate the performance of the model\n\n## Dataset\n- `yelp-dataset-reviews-encoded` (Kaggle: https://www.kaggle.com/datasets/gabrielmadigan/yelp-dataset-reviews-encoded)\n- `yelp-dataset-reviews-nlp` (Kaggle: https://www.kaggle.com/datasets/gabrielmadigan/yelp-dataset-reviews-nlp)\n- Derived from: `yelp-dataset`\n    - Kaggle: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\n    - Yelp: https://www.yelp.com/dataset\n    - Description from the kaggle dataset page:\n    > This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the most recent dataset you'll find information about businesses across 8 metropolitan areas in the USA and Canada.\n- This dataset has been prepared for this module following cleaning, reduction, natural language processing, and numerical encoding of the original Yelp dataset.\n\n## Libraries\n\n- **NumPy** \n    - A fundamental Python package for performing scientific analysis and computation, used to transform data\n    - Webpage: https://numpy.org/\n- **Pandas** \n    - Used to read, load, store, inspect, process, and save the data\n    - Webpage: https://pandas.pydata.org/\n- **Keras** \n    - A powerful library for training deep learning models used to train the classifier \n    - Webpage: https://keras.io/\n\n## Output\n- Model states are saved as `.keras` files after each training epoch \n- Naming convention for files is `model_checkpoint_epochXX_lossY.YY.keras` where `XX` is the number epochs the model was trained on and `Y.YY` is the validation loss","metadata":{}},{"cell_type":"markdown","source":"# 3. Import libraries\n___\n\n- I use `numpy` to pass data as `ndarray` objects to our model\n- I will be importing `pandas` to read, load, and handle the data\n- For plotting during evaluation I'll make use of the `matplotlib` and `seaborn` libraries\n- The `wordcloud` library will allow us to construct a word cloud from the review text for each predicted class for validation\n- I only import `sklearn` to split the data into training and testing sets\n- The `keras` library will be used to build, train, and evaluate the classifier\n- The `tqdm` package is useful for displaying progress bars while processing data (https://tqdm.github.io/)\n- The module from `IPython` ensures every command in a cell is displayed, which saves me from having to write lots of print statements.","metadata":{}},{"cell_type":"code","source":"# Libraries for reading, handling, manipulating and visualizing data\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\n\n# keras modules for training and evaluating neural networks\nimport keras\nfrom keras import Sequential, Input, metrics\nfrom keras.layers import LSTM, Dense\nfrom keras.callbacks import ModelCheckpoint\n\n# Settings for displaying commands in a cell\nfrom IPython.core.interactiveshell import InteractiveShell","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-03T16:38:41.910767Z","iopub.execute_input":"2024-06-03T16:38:41.911170Z","iopub.status.idle":"2024-06-03T16:38:55.626219Z","shell.execute_reply.started":"2024-06-03T16:38:41.911135Z","shell.execute_reply":"2024-06-03T16:38:55.625186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can ignore the warning above\n- Some settings for the notebook that aid with analysis","metadata":{}},{"cell_type":"code","source":"# Display output of every command in a cell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# set default seaborn theme for plots\n#sns.set_theme()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:38:55.628484Z","iopub.execute_input":"2024-06-03T16:38:55.629261Z","iopub.status.idle":"2024-06-03T16:38:55.633028Z","shell.execute_reply.started":"2024-06-03T16:38:55.629196Z","shell.execute_reply":"2024-06-03T16:38:55.632324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Read data\n___\n\n- We will read in two files: the encoded data from the `yelp-dataset-reviews-encoded` dataset and the lemmatized data from `yelp-dataset-reviews-nlp` dataset\n- The former contains the encoded data needed for training and evaluating the model\n- The latter is only used to help sanity-check the performance of the model","metadata":{}},{"cell_type":"code","source":"# Read and load the encoded Yelp review data as a pandas dataframe\ndf_encoded = pd.read_json('/kaggle/input/yelp-dataset-reviews-encoded/encoded_data.json', lines=True)\n\n#  Read and load the lemmatized Yelp review data as a pandas dataframe\ndf_lemmatized = pd.read_json('/kaggle/input/yelp-dataset-reviews-nlp/lemmatized_data.json', lines=True)\n\n# Combine the two dataframes\ndf = pd.concat([df_lemmatized, df_encoded], axis=1)\n\n# Let's also add a label column to our df\ndf['label'] = np.where(df['stars'] <= 3, 'negative', 'positive')\n\n# Inspect the size and data-types of the df\ndf.info()\n\n# Inspect the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:38:55.634266Z","iopub.execute_input":"2024-06-03T16:38:55.634574Z","iopub.status.idle":"2024-06-03T16:40:15.796514Z","shell.execute_reply.started":"2024-06-03T16:38:55.634547Z","shell.execute_reply":"2024-06-03T16:40:15.795411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dataset correctly contains 20k entries\n- The data have been processed (cleaning, subsampling, tokenization, etc.)\n- Text data have been lemmatized and encoded as sequences (uniform in length) of word embeddings\n- Labels have been one-hot encoded corresponding to \"positive\" (4-5 stars) and \"negative\" (1-3 stars) sentiment","metadata":{}},{"cell_type":"markdown","source":"# 5. Data splitting\n___\n\n- Prior to training, we split the data into two sets: a training set and a testing set\n- The training set will of course be passed to the model during training (where the weights will be updated)\n- The testing set will be passed to the model while in a frozen state, without updating the weights, to asses the performance of the trained model on new data\n- The split will be 8/2 training/testing\n- We will also shuffle the data, but using a seed value to guarentee reproducibility","metadata":{}},{"cell_type":"code","source":"TEST_SPLIT = 0.2\nRANDOM_SEED = 0\ndf_train, df_test = train_test_split(df, test_size=TEST_SPLIT, shuffle=True, random_state=RANDOM_SEED)\n\ndf_train.head()\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:15.798985Z","iopub.execute_input":"2024-06-03T16:40:15.799349Z","iopub.status.idle":"2024-06-03T16:40:17.073596Z","shell.execute_reply.started":"2024-06-03T16:40:15.799321Z","shell.execute_reply":"2024-06-03T16:40:17.072585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Keras requires us to pass the data as numpy arrays rather than pandas dataframes, so let's create numpy arrays for our input and label data\n- N.B. The data will remain in their shuffled order, which ensures that we can merge the model outputs back into our original shuffled dataframes for evaluation","metadata":{}},{"cell_type":"code","source":"# Convert encoded inputs and labels from pandas series to numpy arrays for training/evaluation\ntrain_input = np.stack(df_train['word_embeddings'].values)\ntrain_labels = np.stack(df_train['onehot_label'].values)\ntest_input = np.stack(df_test['word_embeddings'].values)\ntest_labels = np.stack(df_test['onehot_label'].values)\n\n# Inspect the numpy arrays\nprint('Training set')\nprint('input data')\ntrain_input.dtype\ntrain_input.shape\n\nprint('label data')\ntrain_labels.dtype\ntrain_labels.shape\n\nprint('Testing set')\nprint('input data:')\ntest_input.dtype\ntest_input.shape\n\nprint('label data:')\ntest_labels.dtype\ntest_labels.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:17.075362Z","iopub.execute_input":"2024-06-03T16:40:17.076143Z","iopub.status.idle":"2024-06-03T16:40:43.100070Z","shell.execute_reply.started":"2024-06-03T16:40:17.076106Z","shell.execute_reply":"2024-06-03T16:40:43.099201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model training\n___\n\n- Now we are ready to build our model\n- We will use a type of Recurrent Neural Network (RNN) called Long Short Term Memory (LSTM)\n- LSTM solves the exploding/vanishing gradient problem seen in vanilla RNNs by tracking long-term memory and short-term memory in parallel, where long-term memory does not get operated on by any weights that might cause it to explode/vanish when performing backpropagation\n- Before we can build our model, we need to set some of the fixed parameters that define our model","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32 # samples in each batch during training and evaluation\nEMBEDDING_DIM = 100 # dimension of each word embedding\nSEQUENCE_LENGTH = 150 # length of padded sequences of word embeddings\nNUM_CLASSES = 2 # number of class labels for one-hot encoding\nLSTM_LAYERS = 100 # number of recurrent layers in the LSTM RNN","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:43.101175Z","iopub.execute_input":"2024-06-03T16:40:43.101465Z","iopub.status.idle":"2024-06-03T16:40:43.105700Z","shell.execute_reply.started":"2024-06-03T16:40:43.101442Z","shell.execute_reply":"2024-06-03T16:40:43.104769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now we can construct our model using a LSTM network\n- We start with a `Sequential` object imported from the Keras library\n- Then we add our input dimensions using the parameters we defined\n- Next, add the LSTM network, specifying the number of recurrent layers\n- Finally, we need to map the output of our LSTM RNN to the target dimension (labels), which we do using a fully-connected dense network\n- The softmax activation function will map the logits of our output into probabilies for each class","metadata":{}},{"cell_type":"code","source":"# Construct a LSTM model\nmodel = Sequential()\nmodel.add(keras.Input(shape=(SEQUENCE_LENGTH, EMBEDDING_DIM)))\nmodel.add(LSTM(LSTM_LAYERS))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:43.107211Z","iopub.execute_input":"2024-06-03T16:40:43.107601Z","iopub.status.idle":"2024-06-03T16:40:43.254731Z","shell.execute_reply.started":"2024-06-03T16:40:43.107574Z","shell.execute_reply":"2024-06-03T16:40:43.253726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- To train our model, we need to select a loss function to minimize after backpropogation and an optimization algorithm to perform when updating the model weights\n- We will use cross-entropy for the loss function as it works well for classification tasks\n- The optimizer we will use is Adam which is widely used and performs well in most situations\n- We will also add an accuracy metric to track the accuracy of the model during training\n- Model accuracy will be determined by assigning the prediction to the class with the highest probability, e.g., if the output is (0.75, 0.25), the first element has the higher probability (0.75) which corresponds to the \"positive\" class (1.0, 0.0)","metadata":{}},{"cell_type":"code","source":"# Compile the model and define the loss function and optimizer\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metrics.CategoricalAccuracy()])","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:43.256348Z","iopub.execute_input":"2024-06-03T16:40:43.256647Z","iopub.status.idle":"2024-06-03T16:40:43.275066Z","shell.execute_reply.started":"2024-06-03T16:40:43.256621Z","shell.execute_reply":"2024-06-03T16:40:43.274234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- One more step: we define the training parameters\n- Epochs determines how many passes over the entire dataset the model will take in training\n- Validation split: the fraction of training data that will be split off to form a validation set\n- We want to save the model at each epoch, so define a \"checkpoint\" to save the model at after each epoch using a keras callback object","metadata":{}},{"cell_type":"code","source":"# Train the model on the training set\nEPOCHS = 20 # Train the model over some number of epochs (passes over entire dataset)\nVALIDATION_SPLIT = 0.2 # Split off a validation set from the training data\n\n# Checkpoint object\ncheckpoint_filepath = '/kaggle/working/model_checkpoint_epoch{epoch:02d}.keras'\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_filepath, \n                                monitor='val_accuracy', \n                                verbose=0, \n                                save_best_only=False, \n                                save_weights_only=False, \n                                mode='auto', \n                                save_freq='epoch',\n                                initial_value_threshold=None,\n                               )","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:43.276695Z","iopub.execute_input":"2024-06-03T16:40:43.277462Z","iopub.status.idle":"2024-06-03T16:40:43.283291Z","shell.execute_reply.started":"2024-06-03T16:40:43.277428Z","shell.execute_reply":"2024-06-03T16:40:43.282458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Finally, we can train the model and track the accuracy","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_input, \n                    train_labels, \n                    batch_size=BATCH_SIZE, \n                    epochs=EPOCHS, \n                    validation_split=VALIDATION_SPLIT, \n                    validation_batch_size=BATCH_SIZE, \n                    callbacks=[model_checkpoint]\n                   )","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:40:43.287477Z","iopub.execute_input":"2024-06-03T16:40:43.287852Z","iopub.status.idle":"2024-06-03T16:53:50.901345Z","shell.execute_reply.started":"2024-06-03T16:40:43.287825Z","shell.execute_reply":"2024-06-03T16:53:50.900186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It will be easier to interpret the model's performance during training with a few plots","metadata":{}},{"cell_type":"code","source":"# Plot the model accuracy per training epoch in training and validation data\n\n# Create a subplot\nfig, ax = plt.subplots(figsize=(10,5))\n\n# Plot the data as a scatter plot, specify data labels for legend\nax.scatter(range(1, EPOCHS+1), history.history['categorical_accuracy'], label='Training', zorder=2)\nax.scatter(range(1, EPOCHS+1), history.history['val_categorical_accuracy'], color='#DC0000', label='Validation', zorder=2)\n\n# Format the plots here\n\n# Set plot title text and location\nax.set_title('LSTM models', loc='right')\n\n# Set x- and y-axis label text and locations\nax.set_xlabel('Training epochs', labelpad=10, fontsize=14, loc='right')\nax.set_ylabel('Accuracy', labelpad=10, fontsize=14, loc='top')\n\n# Add minor ticks\nax.minorticks_on()\n\n# Set the length of x- and y-axis major and minor ticks, position the ticks inside the plot, and add ticks to the top and right side of the plot\nax.tick_params(axis=\"x\", which='major', direction='in', length=8, right=True, top=True)\nax.tick_params(axis=\"x\", which='minor', direction='in', length=4, right=True, top=True)\nax.tick_params(axis=\"y\", which='major', direction='in', length=8, right=True, top=True)\nax.tick_params(axis=\"y\", which='minor', direction='in', length=4, right=True, top=True)\n\n# Set which x- and y-axis values to display (and major ticks, correspondingly)\nax.locator_params(axis='y', nbins=6)\nax.set_xticks(np.linspace(0, 20, 11).astype(int))\n\n# Add grid lines\nax.grid(which='major', axis='x', color='black', linestyle='dotted', alpha=1.0, zorder=0)\nax.grid(which='major', axis='y', color='black', linestyle='dotted', alpha=1.0, zorder=0)\n\n# Add a legend\nax.legend(fancybox=False, framealpha=1.0, edgecolor='black');\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:50.903535Z","iopub.execute_input":"2024-06-03T16:53:50.907719Z","iopub.status.idle":"2024-06-03T16:53:51.600481Z","shell.execute_reply.started":"2024-06-03T16:53:50.907689Z","shell.execute_reply":"2024-06-03T16:53:51.599368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the model's loss per training epoch in training and validation data\n\n# Create a subplot\nfig, ax = plt.subplots(figsize=(10,5))\n\n# Plot the data as a line, specify data labels for legend\nax.scatter(range(1, EPOCHS+1), history.history['loss'], label='Training', zorder=2)\nax.scatter(range(1, EPOCHS+1), history.history['val_loss'], color='#DC0000', label='Validation', zorder=2)\n\n# Format the plots here\n\n# Set plot title text and location\nax.set_title('LSTM models', loc='right')\n\n# Set x- and y-axis label text and locations\nax.set_xlabel('Training epochs', labelpad=10, fontsize=14, loc='right')\nax.set_ylabel('Loss', labelpad=10, fontsize=14, loc='top')\n\n# Add minor ticks\nax.minorticks_on()\n\n# Set the length of x- and y-axis major and minor ticks, position the ticks inside the plot, and add ticks to the top and right side of the plot\nax.tick_params(axis=\"x\", which='major', direction='in', length=8, right=True, top=True)\nax.tick_params(axis=\"x\", which='minor', direction='in', length=4, right=True, top=True)\nax.tick_params(axis=\"y\", which='major', direction='in', length=8, right=True, top=True)\nax.tick_params(axis=\"y\", which='minor', direction='in', length=4, right=True, top=True)\n\n# Set which x- and y-axis values to display (and major ticks, correspondingly)\nax.locator_params(axis='y', nbins=6)\nax.set_xticks(np.linspace(0, 20, 11).astype(int))\n\n# Add grid lines\nax.grid(which='major', axis='x', color='black', linestyle='dotted', alpha=1.0, zorder=0)\nax.grid(which='major', axis='y', color='black', linestyle='dotted', alpha=1.0, zorder=0)\n\n# Add a legend\nax.legend(fancybox=False, framealpha=1.0, edgecolor='black');","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:51.601954Z","iopub.execute_input":"2024-06-03T16:53:51.602399Z","iopub.status.idle":"2024-06-03T16:53:52.257830Z","shell.execute_reply.started":"2024-06-03T16:53:51.602362Z","shell.execute_reply":"2024-06-03T16:53:52.256939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see the model accuracy improve in training data with each epoch before plateauing at around epoch 11\n- The model accuracy with the validation set peaks at 3 epochs and then gradually declines\n- This implies that there is strong bias toward training data in the model after 3 or so training epochs\n- Likewise, looking at the loss after each epoch, we see that in training it continues to decrease (output more closely matches labels), while in the validation set the loss dips at 3 epochs and then continues to increase with more training epochs, further indicating that the model suffers from training bias after 3 epochs\n- Therefore the best performing model is the one trained on 3 epochs\n- Let's see how this model performs with the testing dataset we set aside","metadata":{}},{"cell_type":"markdown","source":"# 6. Model evaluation\n___\n\n- Time to evaluate our model\n- Load the model state (weights and biases) saved after 3 training epochs back into the model","metadata":{}},{"cell_type":"code","source":"# Load the saved model weights and biases\nbest_model_filepath = '/kaggle/working/model_checkpoint_epoch03.keras'\nmodel.load_weights(best_model_filepath)\n\n# Evaluate the model with the testing dataset\nmodel.evaluate(test_input, test_labels, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:24:48.073949Z","iopub.execute_input":"2024-06-03T18:24:48.074719Z","iopub.status.idle":"2024-06-03T18:24:53.686499Z","shell.execute_reply.started":"2024-06-03T18:24:48.074681Z","shell.execute_reply":"2024-06-03T18:24:53.685383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Evaluating the model (trained with 4 epochs) with the test dataset returned an accuracy of 87.5 % which closely matches the accuracy of the validation set during training (88%)\n- While not perfect, a testing accuracy of 87% shows that this is a particularly well-performing model!\n- Now let's get the model's predictions with the training and testing sets for further evaluation","metadata":{}},{"cell_type":"code","source":"# Get the predictions with our model for training and testing sets\ntrain_pred = model.predict(train_input, batch_size=BATCH_SIZE)\ntest_pred = model.predict(test_input, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:53:57.857278Z","iopub.execute_input":"2024-06-03T16:53:57.858953Z","iopub.status.idle":"2024-06-03T16:54:23.568833Z","shell.execute_reply.started":"2024-06-03T16:53:57.858909Z","shell.execute_reply":"2024-06-03T16:54:23.567697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Now add the predictions back into the training and testing (split and shuffled) dataframes","metadata":{}},{"cell_type":"code","source":"# Load the predictions into new dataframes (one for training one for testing)\ndf_train_pred = pd.DataFrame(train_pred, columns=['pos_pred', 'neg_pred'])\ndf_test_pred = pd.DataFrame(test_pred, columns=['pos_pred', 'neg_pred'])\n\n# Add predictions as columns to training/testing dataframes\n# The ordering of the predictions still matches the original shuffled train/test datasets if we convert to a list\ndf_train['pos_pred'] = list(df_train_pred['pos_pred'])\ndf_train['neg_pred'] = list(df_train_pred['neg_pred'])\ndf_test['pos_pred'] = list(df_test_pred['pos_pred'])\ndf_test['neg_pred'] = list(df_test_pred['neg_pred'])\n\n# Now add the predicted sentiment label, based on a prediction threshold of 0.5\ndf_train['pred_label'] = np.where(df_train['pos_pred'] > 0.5, 'positive', 'negative')\ndf_test['pred_label'] = np.where(df_test['pos_pred'] > 0.5, 'positive', 'negative')\n\n# Verify the size and data-types of the df\ndf_train.info()\ndf_test.info()\n\n# Verify the first few rows\ndf_train.head()\ndf_test.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:54:23.571519Z","iopub.execute_input":"2024-06-03T16:54:23.571864Z","iopub.status.idle":"2024-06-03T16:54:24.905013Z","shell.execute_reply.started":"2024-06-03T16:54:23.571838Z","shell.execute_reply":"2024-06-03T16:54:24.903949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It is helpful to look at the distribution of prediction values (probabilities) for each class to see how \"confident\" the model is in predicting each class","metadata":{}},{"cell_type":"code","source":"# Plot the distribution of predictions (probabilities) for positive and negative sentiment data\n\n# Create subplots with 2 rows\nfig, ax = plt.subplots(nrows=2, sharex=True, figsize=(10,6))\n\n# Plot the data as step histograms with 50 bins, a log-scale y-axis, and specify data labels for legend\n# Training data\nax[0].hist(df_train.loc[df_train['label'] == 'positive']['pos_pred'], bins=50, log=True, histtype=u'step', label='Positive reviews') # positive data\nax[0].hist(df_train.loc[df_train['label'] == 'negative']['neg_pred'], bins=50, log=True, histtype=u'step', color='#DC0000', label='Negative reviews') # negative data\n\n# Testing data\nax[1].hist(df_test.loc[df_test['label'] == 'positive']['pos_pred'], bins=50, log=True,  histtype=u'step', label='Positive reviews') # positive data\nax[1].hist(df_test.loc[df_test['label'] == 'negative']['neg_pred'], bins=50, log=True, histtype=u'step', color='#DC0000', label='Negative reviews') # negative data\n\n# Format the plots here\n\n# Set plot title text and location\nax[0].set_title('LSTM model (3 epochs)', loc='right')\n\n# Set x- and y-axis label text and locations\nax[1].set_xlabel('Sentiment prediction (probability)', labelpad=10, fontsize=14, loc='right')\nax[0].set_ylabel('Frequency', labelpad=10, fontsize=14, loc='top')\nax[1].set_ylabel('Frequency', labelpad=10, fontsize=14, loc='top')\n\n# Add minor ticks\nax[0].minorticks_on()\nax[1].minorticks_on()\n\n# Set the length of x- and y-axis major and minor ticks, position the ticks inside the plot, and add ticks to the top and right side of the plot\nax[0].tick_params(axis=\"x\", which='major', direction='in', length=8, right=True, top=True)\nax[0].tick_params(axis=\"x\", which='minor', direction='in', length=4, right=True, top=True)\nax[0].tick_params(axis=\"y\", which='major', direction='in', length=8, right=True, top=True)\nax[0].tick_params(axis=\"y\", which='minor', direction='in', length=4, right=True, top=True)\nax[1].tick_params(axis=\"x\", which='major', direction='in', length=8, right=True, top=True)\nax[1].tick_params(axis=\"x\", which='minor', direction='in', length=4, right=True, top=True)\nax[1].tick_params(axis=\"y\", which='major', direction='in', length=8, right=True, top=True)\nax[1].tick_params(axis=\"y\", which='minor', direction='in', length=4, right=True, top=True)\n\n# Set which x- and y-axis values to display (and major ticks, correspondingly)\n#ax.locator_params(axis='y', nbins=6)\nax[0].set_xticks(np.linspace(0, 1, 11))\nax[1].set_xticks(np.linspace(0, 1, 11))\n\n# Set x-axis range to display\nax[0].set_xlim(0, 1)\nax[1].set_xlim(0, 1)\n\n# Add grid lines\nax[0].grid(which='major', axis='x', color='black', linestyle='dotted', alpha=1.0, zorder=0)\nax[0].grid(which='major', axis='y', color='black', linestyle='dotted', alpha=1.0, zorder=0)\nax[1].grid(which='major', axis='x', color='black', linestyle='dotted', alpha=1.0, zorder=0)\nax[1].grid(which='major', axis='y', color='black', linestyle='dotted', alpha=1.0, zorder=0)\n\n# Add a legend\nax[0].legend(title='Training data', fancybox=False, framealpha=1.0, edgecolor='black')\nax[1].legend(title='Testing data', fancybox=False, framealpha=1.0, edgecolor='black');","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:35:02.370239Z","iopub.execute_input":"2024-06-03T18:35:02.371104Z","iopub.status.idle":"2024-06-03T18:35:04.174352Z","shell.execute_reply.started":"2024-06-03T18:35:02.371055Z","shell.execute_reply":"2024-06-03T18:35:04.173228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The distributions are strongly peaked toward a prediction probability of 1.0, indicating that the model performs well for both classes and on both datasets \n- There are more negative reviews predicted with low probability than there are positive (in the left tails), and vis versa for predictions with high probability (right peaks), indicating that the model performs better on positive reviews than negative\n- This is expected as the data were biased toward posititve reviews","metadata":{}},{"cell_type":"markdown","source":"## Class accuracy, error, and confusion matrix\n\n- We can turn our model into a binary classifier by assigning a review to the class that recieves a prediction greater than 0.5 (this was done automatically by Keras when computing the training, validation, and evaluation accuracies)\n- Using this threshold we can compute the accuracy and error for each class, and a confusion matrix","metadata":{}},{"cell_type":"code","source":"# Confusion matrix, accuracy, and erro for testing data\n\nT = len(df_test) # total count (P + N)\nP = len(df_test.loc[df_test['label'] == 'positive']) # Ground truth positive count\nN = len(df_test.loc[df_test['label'] == 'negative']) # Ground truth negative count\nPP = len(df_test.loc[df_test['pred_label'] == 'positive']) # Predicted positive count\nPN = len(df_test.loc[df_test['pred_label'] == 'negative']) # Predicted negative count\nTP = len(df_test.loc[(df_test['pred_label'] == 'positive') & (df_test['label'] == 'positive')]) # True positive count\nFP = len(df_test.loc[(df_test['pred_label'] == 'positive') & (df_test['label'] == 'negative')]) # False positive count\nTN = len(df_test.loc[(df_test['pred_label'] == 'negative') & (df_test['label'] == 'negative')]) # True negative count\nFN = len(df_test.loc[(df_test['pred_label'] == 'negative') & (df_test['label'] == 'positive')]) # False negative count\n\nT_rate = T/T # total rate (1.0)\nP_rate = P/T # Grount truth positive rate\nN_rate = N/T # Ground truth negative rate\nPP_rate = PP/T # Predicted positive rate\nPN_rate = PN/T # Predicted negative rate\nTP_rate = TP/P # True positive rate\nFP_rate = FP/N # False positive rate\nTN_rate = TN/N # True negative rate\nFN_rate = FN/P # False negative rate\n\n# Confusion matrix of counts\nconfusion_matrix_counts = np.array([[T, PP, PN],\n                                    [P, TP, FN],\n                                    [N, FP ,TN]])\n\n# Confusion matrix of rates\nconfusion_matrix_rates = np.array([[T_rate, PP_rate, PN_rate],\n                                   [P_rate, TP_rate, FN_rate],\n                                   [N_rate, FP_rate, TN_rate]])\n\n# Let's also look at the accuracy and error for each class\nT_acc = (TP+TN)/T # Total accuracy\nT_err = (FP+FN)/T # Total error rate\n\n# Print accuracy and errors\nprint(f'Model accuracy: {100*T_acc:.2f}%')\nprint(f'Model error: {100*T_err:.2f}%')\n\n# Plot the confusion matrix of counts/rates\n\n# Get the labels for each square\ncm_names = ['Total', 'Positive', 'Negative', 'Positive', 'True Positive', 'False Negative', 'Negative', 'False Positive', 'True Negative']\ncm_counts = [f'{count}' for count in confusion_matrix_counts.flatten()]\ncm_rates = [f'{100*rate:.0f}%' for rate in confusion_matrix_rates.flatten()]\ncm_labels = [ f'{name}\\n{count}\\n{rate}' for name, count, rate in zip(cm_names, cm_counts, cm_rates)]\ncm_labels = np.asarray(cm_labels).reshape(3,3)\n\n# Create a subplot\nf, ax = plt.subplots(figsize=(6,6))\n\n# Plot as a heatmap with annotations/values inside each square, with no sidebar\nsns.heatmap(confusion_matrix_counts, annot=cm_labels, cmap='Blues', fmt='', cbar=False)\n\n# Do not display x- and y-axis ticks/values\nax.set_xticks([])\nax.set_yticks([])\n\n# Set the x- and y-axis labels\nax.set_xlabel('Predicted sentiment', labelpad=10, fontsize=14, loc='center')\nax.set_ylabel('True sentiment', labelpad=10, fontsize=14, loc='center');","metadata":{"execution":{"iopub.status.busy":"2024-06-03T18:47:11.240426Z","iopub.execute_input":"2024-06-03T18:47:11.240862Z","iopub.status.idle":"2024-06-03T18:47:11.516526Z","shell.execute_reply.started":"2024-06-03T18:47:11.240829Z","shell.execute_reply":"2024-06-03T18:47:11.515415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The model accuracy on test documents with positive sentiment is 94% while the error rate is 6%\n- The model accuracy on test documents with negative sentiment is 73% while the error rate is 27%\n- The model accuracy on test documents of any sentimnet is 87.5% while the error rate is 12.5% (consistent with the Keras evaluate method)","metadata":{}},{"cell_type":"markdown","source":"# Word Cloud\n\n- As a final sanity-check, let's make a word cloud for all the text in each class prediction and see if this matches what we might expect","metadata":{}},{"cell_type":"code","source":"# Join all the text into one string for each subset\ntext_pred_pos = ' '.join([' '.join(word) for word in list(df_test.loc[df_test['pred_label'] == 'positive']['lemmatized_text'])])\ntext_pred_neg = ' '.join([' '.join(word) for word in list(df_test.loc[df_test['pred_label'] == 'negative']['lemmatized_text'])])\ntext_label_pos = ' '.join([' '.join(word) for word in list(df_test.loc[df_test['label'] == 'positive']['lemmatized_text'])])\ntext_label_neg = ' '.join([' '.join(word) for word in list(df_test.loc[df_test['label'] == 'negative']['lemmatized_text'])])\n\n# Create the word clouds for each subset\nwc_pred_pos = WordCloud(background_color='white', colormap = 'RdBu').generate(text_pred_pos)\nwc_pred_neg = WordCloud(background_color='white', colormap = 'RdBu').generate(text_pred_neg)\nwc_label_pos = WordCloud(background_color='white', colormap = 'RdBu').generate(text_label_pos)\nwc_label_neg = WordCloud(background_color='white', colormap = 'RdBu').generate(text_label_neg)\n\n# Plotting\n\n# Creat 4 subplots\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, sharex=True, figsize=(10, 6))\n\n# Plot the word clouds\nax[0,0].imshow(wc_label_pos)\nax[0,1].imshow(wc_pred_pos)\nax[1,0].imshow(wc_label_neg)\nax[1,1].imshow(wc_pred_neg)\n\n# Set subplot titles text and location\nax[0,0].set_title('Positive sentiment (labeled)', loc='center')\nax[0,1].set_title('Positive sentiment (predicted)', loc='center')\nax[1,0].set_title('Negative sentiment (labeled)', loc='center')\nax[1,1].set_title('Negative sentiment (predicted)', loc='center')\n\n# Turn off the axes\nax[0,0].axis(\"off\")\nax[0,1].axis(\"off\")\nax[1,0].axis(\"off\")\nax[1,1].axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T16:54:27.239656Z","iopub.execute_input":"2024-06-03T16:54:27.239991Z","iopub.status.idle":"2024-06-03T16:54:33.727424Z","shell.execute_reply.started":"2024-06-03T16:54:27.239963Z","shell.execute_reply":"2024-06-03T16:54:33.726495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The word clouds show that the most frequent words do not vary much between predicted and actual labels\n- It is very clear that the positive sentiment includes words we would expect, like \"good,\" \"love,\" \"amazing,\" and \"delicious\"\n- The negative sentiment is harder to interpret---we still see expected words like \"bad,\" and \"wait,\"---but we see more neutral than negative, like \"food,\" \"place,\" \"service,\" and even positive words like \"good\"\n- However, this makes some sense, as we included 3 star reviews in the negative class which could be seen as more neutral than negative, and even 1 and 2 star reviews may not carry as strong a sentiment as the positive reviews (despite being associated with such a low star rating)\n- Also, there is far fewer data in the negative class than the positive which can impact these results","metadata":{}},{"cell_type":"markdown","source":"# 7. Discussion\n___\n\nOverall this project was a great success! The best model is able to achieve an overall accuracy of 87% which is decent performance given the training sample size and the limitations introduced by training on Yelp business reviews. In order to improve the model, several steps could be taken. \n1. Take a larger sample from the original dataset. I only used a small fraction to expidite the analysis, but given more time and memory resources I believe more training samples would improve the model accuracy. Also, selecting a subset with a uniform distribution of positive and negative sentiment would remove the heavy bias toward positive sentiment, if that was of interest. \n2. More fine tuning of the NLP. For example, using a larger corpus for the word2vec training or increasing the size of the word embeddings. \n3. Fine tune the model and training parameters. While I optimized on the number of training epochs, other parameters could be batch size and step size when finding the loss function minima. Also, I could experiment with other optimization algorithms and loss functions, although the Adam optimizer and cross-entropy are usually well suited for this type of task. For the model, the number of recurrent layers could potentially be optimized, along with the fully-connected layers. \n4. Changing the class definitions. I could experiment with how many stars are associated with each class, or even introduce a neutral class. This comes with additional challenges, and I think Yelp reviews are not the best source of text for that level of nuance. \n5. Removing high-frequency neutral words. As I noticed with the word clouds, both classes were still mostly neutral in tone. The words \"place,\" \"food,\" and \"time\" all appeared with very high frequency in all reviews. Removing these words from the text (like stop words) could potentially improve the performance of the model.\n6. Finally, and what I think would offer the greatest improvement, is improve the data source and quality. Yelp reviews are not as well written as published text like Wikipedia, journals, books, etc., and include lots of typos, misspellings, odd/repeated punctuation, etc., which can confuse the spaCy NLP model. Better data cleaning can help but there will always be limits. The limited context of a Yelp review means there is not much variation in the content of the texts (this is most obvious when looking at the word clouds). For example, sentiment will almost exclusively involve either food taste, wait times, or service quality, which will bias the model. A corpus with a greater scope could result in a better model, e.g., IMDB movie reviews or Twitter/X tweets.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}