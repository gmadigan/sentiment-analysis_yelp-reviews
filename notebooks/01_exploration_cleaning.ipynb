{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3316532,"sourceType":"datasetVersion","datasetId":10100}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Module 1 - Data Preparation*\n---","metadata":{}},{"cell_type":"markdown","source":"# 1. Project summary\n___\n\nThis is part of a project to build a sentiment classifier trained on Yelp review data (https://www.yelp.com/dataset). The project has been divided into several modules to perform different parts of the analysis, e.g., data cleaning, data processing, and model training. The goal is to predict the sentiment of a document; while using Yelp reviews of businesses, the 1-5 star rating acts as a proxy for sentiment, and the written Yelp review as the document text. The project is written in Python on Jupyter notebooks and makes use of a range of data science tools like pandas, spaCy, word2vec, and keras. My motivation in starting this project is to build my skillset, learn new tools, and improve as a data scientist. It is an ongoing project and may see many updates/iterations.","metadata":{}},{"cell_type":"markdown","source":"# 2. Module Overview\n___\n\n## Goal\n- The goal of this module is to prepare Yelp review data for eventual training and evaluation of a sentiment classifier\n    - Data Exploration:\n        - The data are inspected to identify structure, data-types, data classes, distributions, missing values, etc.\n    - Data Reduction:\n        - Only a representative subsample of the dataset is kept for more efficient data analysis\n        - Irrelevant features are removed (e.g., date, business ID)\n    - Data Cleaning:\n        - Data are cleaned to ensure no missing values, duplicates, etc.\n        - Minimal cleaning of the review text is performed, e.g., replacing odd whitespaces, backslashes, etc.\n- The above steps are organized for clarity but are not performed in the exact order listed\n- Prepared data are saved in an ouput json file for processing by the next module\n\n## Data\n- Input Data:\n    - `yelp-dataset`\n    - Kaggle: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\n    - Yelp: https://www.yelp.com/dataset\n- Description from the kaggle dataset page:\n> This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the most recent dataset you'll find information about businesses across 8 metropolitan areas in the USA and Canada.\n- Specifically, I analyze the `yelp-dataset/yelp_academic_dataset_review.json` file which contains data pertaining to Yelp user reviews and includes the text of the review and an associated 1-5 star rating of the Yelp user's experience (among other data, e.g., date, business ID, etc.)\n- Data are indexed per review with no discernable sorting\n\n## Libraries\n- Key Libraries:\n    - `Pandas` - used to read, load, store, inspect, process, and save the data\n        - webpage: https://pandas.pydata.org/\n\n## Output\n- Output:\n    - `/kaggle/working/cleaned_reduced_data.json`\n- This json file contains a reduced dataset of the input data that has been cleaned and prepared for NLP","metadata":{}},{"cell_type":"markdown","source":"# 3. Import Libraries\n___\n\n- I will only be using `pandas` at this stage\n- The module from `IPython` ensures every command in a cell is displayed, which saves me from having to write lots of print statements","metadata":{}},{"cell_type":"code","source":"# Libraries for reading, handling, and visualizing data\nimport pandas as pd\nimport seaborn as sns\n\n# For quickly getting file lengths\nfrom subprocess import check_output\n\n# Settings for displaying commands in a cell\nfrom IPython.core.interactiveshell import InteractiveShell","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-30T21:55:55.247534Z","iopub.execute_input":"2024-05-30T21:55:55.247905Z","iopub.status.idle":"2024-05-30T21:55:55.253701Z","shell.execute_reply.started":"2024-05-30T21:55:55.247872Z","shell.execute_reply":"2024-05-30T21:55:55.252158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Some settings for the notebook that aid with analysis","metadata":{}},{"cell_type":"code","source":"# Display output of every command in a cell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# Set default seaborn theme for plots\nsns.set_theme()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:55:55.257177Z","iopub.execute_input":"2024-05-30T21:55:55.258084Z","iopub.status.idle":"2024-05-30T21:55:55.268589Z","shell.execute_reply.started":"2024-05-30T21:55:55.258041Z","shell.execute_reply":"2024-05-30T21:55:55.267331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Exploration, Reduction, and Cleaning\n___\n\n## Data exploration - file size, structure\n\n- I'll now explore the data, reducing and cleaning in situ\n- Start by opening the input file `/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json`\n- Read the first line to get a sense for how the file is structured","metadata":{}},{"cell_type":"code","source":"# Inspect the structure of the Yelp review data json file using a single entry\nINPUT_FILE = '/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json'\nwith open(INPUT_FILE, 'r') as json_file:\n    json_file.readline()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:55:55.270458Z","iopub.execute_input":"2024-05-30T21:55:55.271075Z","iopub.status.idle":"2024-05-30T21:55:55.290452Z","shell.execute_reply.started":"2024-05-30T21:55:55.271034Z","shell.execute_reply":"2024-05-30T21:55:55.289094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- It is apperent that a single line is a self-contained json object corresponding to a single observation (review)\n- The keys are data labels and the values are data values\n- Let's see how many observations this file contains","metadata":{}},{"cell_type":"code","source":"# Get the number of lines in the json file, equal to the number of data observations (reviews)\nint(check_output(['wc', '-l', INPUT_FILE]).split()[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:55:55.293022Z","iopub.execute_input":"2024-05-30T21:55:55.293455Z","iopub.status.idle":"2024-05-30T21:56:48.841927Z","shell.execute_reply.started":"2024-05-30T21:55:55.293414Z","shell.execute_reply":"2024-05-30T21:56:48.840606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Almost 7 million reviews! This is far more than I will need for analysis and will slow the processing down considerably\n- Before I take a subsample, let's make sure that the reviews are not ordered in any meaningful way\n- This lets us just truncate the data rather than having to randomly sample it which can take time (pass through first m < n lines rather than all n lines)","metadata":{}},{"cell_type":"code","source":"# Read and inspect the first few lines of the json file to see if review data are shuffled\n# Use a pandas dataframe for easier inspection\ndf_head_10 = pd.read_json(INPUT_FILE, lines=True, nrows=10)\ndf_head_10.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:48.843656Z","iopub.execute_input":"2024-05-30T21:56:48.844485Z","iopub.status.idle":"2024-05-30T21:56:48.892372Z","shell.execute_reply.started":"2024-05-30T21:56:48.844442Z","shell.execute_reply":"2024-05-30T21:56:48.891356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There does not appear to be sorting in any column, so we will assume the data are properly shuffled","metadata":{}},{"cell_type":"markdown","source":"## Data reduction - subsampling\n\n- I will truncate the number of lines read so we only load a representative subsample of the total data\n- Read a subsample of the data from the input file `/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json` and load into a pandas dataframe","metadata":{}},{"cell_type":"code","source":"# Read the Yelp review data\n# Dataset is large (~7 million reviews!) \n# Read the first 20,000 lines of the input file and load the data subsample into a pandas dataframe for analysis \nREAD_SIZE = 20000\ndf = pd.read_json(INPUT_FILE, lines=True, nrows=READ_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:48.896204Z","iopub.execute_input":"2024-05-30T21:56:48.896896Z","iopub.status.idle":"2024-05-30T21:56:49.256677Z","shell.execute_reply.started":"2024-05-30T21:56:48.896861Z","shell.execute_reply":"2024-05-30T21:56:49.255517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data exploration - structure, dtypes, null values, duplicates\n\n- After reading and loading the data subsample, I inspect the contents of the dataframe: identify the data-types, check for null entries and duplicates, and peek the first few rows of data","metadata":{}},{"cell_type":"code","source":"# Inspect the size and data-types of the df\ndf.info()\n\n# Check for null/missing values\ndf.isnull().values.any()\n\n# Check for any duplicate entries\ndf.duplicated().values.any()\n\n# Inspect the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.258436Z","iopub.execute_input":"2024-05-30T21:56:49.258736Z","iopub.status.idle":"2024-05-30T21:56:49.361203Z","shell.execute_reply.started":"2024-05-30T21:56:49.258710Z","shell.execute_reply":"2024-05-30T21:56:49.360106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Fortunately there are no null-valued or duplicate entries in our sample\n- We can see there are 9 columns with int and str data-types","metadata":{}},{"cell_type":"markdown","source":"## Data reduction - feature selection\n\n- I am only interested in the \"text\" and \"stars\" columns for the purposes of sentiment analysis\n- Remove all but these two columns from the dataframe","metadata":{}},{"cell_type":"code","source":"# Remove columns we will not need from the df\n# For building a sentiment classifier we will only need to keep:\n# 1. Written reviews (\"text\") for training features\n# 2. Star ratings (\"stars\") for training targets\ndf = df[['stars','text']]","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.362588Z","iopub.execute_input":"2024-05-30T21:56:49.362910Z","iopub.status.idle":"2024-05-30T21:56:49.370295Z","shell.execute_reply.started":"2024-05-30T21:56:49.362881Z","shell.execute_reply":"2024-05-30T21:56:49.369316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data exploration - class distributions\n- Let's explore the star ratings in the 'stars' column, which will serve as our categories or class labels\n- Start by inspecting the 'stars' column to understand the values","metadata":{}},{"cell_type":"code","source":"# Check the values of the class labels (\"stars\")\nsorted(pd.unique(df['stars']))\nlen(pd.unique(df['stars']))","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.371495Z","iopub.execute_input":"2024-05-30T21:56:49.371794Z","iopub.status.idle":"2024-05-30T21:56:49.387591Z","shell.execute_reply.started":"2024-05-30T21:56:49.371769Z","shell.execute_reply":"2024-05-30T21:56:49.386613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see the values are integers ranging from 1 to 5\n- This might seem obvious, but it's good to check since these will become our class labels\n- Now look at how these values are distributed","metadata":{}},{"cell_type":"code","source":"NUM_CLASSES = 5\nMIN_STAR_VAL = 1\nMAX_STAR_VAL = 5\n\n# Inspect the distribution of classes in the dataset\ndf['stars'].value_counts(ascending=True)\ndf['stars'].plot.hist(bins=NUM_CLASSES, range=(MIN_STAR_VAL-.5,MAX_STAR_VAL+.5), xlabel='stars')","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.389329Z","iopub.execute_input":"2024-05-30T21:56:49.389647Z","iopub.status.idle":"2024-05-30T21:56:49.767349Z","shell.execute_reply.started":"2024-05-30T21:56:49.389620Z","shell.execute_reply":"2024-05-30T21:56:49.766234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The data are heavily bias toward 5 star ratings, followed by 4 stars.\n- There are slightly more 1 star ratings than 2 star\n- This is not very surprising: people are more inclined to leave positive reviews after a great experience with a business, and are more likely to give strong statements like 5 stars or 1 stars, than neutral statements, say, 2 or 3 stars\n- I am slightly surprised there are not more 1 star reviews, as I imagine many people feel inclined to leave feedback after a very negative experience\n- Based on a cursury read of some of the more negative sounding reviews, is seems as though negative reviews are just distributed among the 1, 2, and 3 star ratings, i.e., even a very negatively written review can be awarded a 2 or even 3 star rating\n- I could reduce the data by taking a subset with a uniform distribution of star ratings, as training a model on data with an uneven class distribution can lead to bias\n- There is also a good reason to keep this distribution, as it is a real feature of user sentiment and we want a model that will reflect that\n- I will retain this data and make that decision later in the analysis\n- This also allows me to group the classes down the road for, say, a binary classifier, with greater data retention","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning - text data\n\n- Now I focus on cleaning the text data, i.e., the written review of the business\n- Start by checking for odd whitespaces including:\n    - newline characters `\\n`\n    - tabs `\\t`\n    - backslashes or escape characters `\\`\n    - double spaces, tripple spaces, etc.","metadata":{}},{"cell_type":"code","source":"# Now let's clean up the textual data\n\n# This let's us see the entire text of each review - better for analysis\npd.set_option('display.max_colwidth', 10000)\n\n# Search for any odd whitespace or backslashes in the text\ndf['text'].str.contains('\\n', regex=True).any() # newline\ndf['text'].str.contains('\\t', regex=True).any() # tab\ndf['text'].str.contains('  ', regex=True).any() # double space\ndf['text'].str.contains(r'\\\\', regex=True).any() # backslash","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.770109Z","iopub.execute_input":"2024-05-30T21:56:49.770810Z","iopub.status.idle":"2024-05-30T21:56:49.863706Z","shell.execute_reply.started":"2024-05-30T21:56:49.770772Z","shell.execute_reply":"2024-05-30T21:56:49.862547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are newline characters, backslashes, and double spaces present\n- Let's take a look at an example of each occurance","metadata":{}},{"cell_type":"code","source":"# Look at a couple examples of each\npd.set_option('display.max_colwidth', 10000) # display the entire review\ndf[df['text'].str.contains('\\n', regex=True) == True]['text'].head(1)\ndf[df['text'].str.contains(r'\\\\', regex=True) == True]['text'].head(1)\ndf[df['text'].str.contains('  ', regex=True) == True]['text'].head(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.864646Z","iopub.execute_input":"2024-05-30T21:56:49.864964Z","iopub.status.idle":"2024-05-30T21:56:49.949394Z","shell.execute_reply.started":"2024-05-30T21:56:49.864936Z","shell.execute_reply":"2024-05-30T21:56:49.948203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- In the first example there are several `\\n` chars\n- In the second example we see a `\\` in \"`4\\5 stars`\" which is likely just a typo that the Yelp user intended as a `/` (also back-to-back `\\n` chars)\n- In the last example, there is a double space, also likely a typo","metadata":{}},{"cell_type":"markdown","source":"- While we could apply different replacements in each case, for the purposes of sentiment analysis it is sufficient to replace all of these whitespaces/backslashes with a single space; it will not drastically change the meaning of the text\n- Replace the whitespaces and add the cleaned text as a new column to the dataframe\n- Verify the cleaned data look as expected\n- N.B. Punctuation and special characters will be dealt with in a separate module, where we use a pretrained English langauge model to tokenize the text","metadata":{}},{"cell_type":"code","source":"# Replace all whitespace and backslashes with a single space\ndf['cleaned_text'] = df['text'].str.replace('\\s+', ' ', regex=True)\ndf['cleaned_text'] = df['cleaned_text'].str.replace(r'\\\\', ' ', regex=True)\n\n# Verify the changes with the same examples\ndf[df['text'].str.contains('\\n', regex=True) == True]['cleaned_text'].head(1)\ndf[df['text'].str.contains(r'\\\\', regex=True) == True]['cleaned_text'].head(1)\ndf[df['text'].str.contains('  ', regex=True) == True]['cleaned_text'].head(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:49.951136Z","iopub.execute_input":"2024-05-30T21:56:49.951544Z","iopub.status.idle":"2024-05-30T21:56:50.789752Z","shell.execute_reply.started":"2024-05-30T21:56:49.951513Z","shell.execute_reply":"2024-05-30T21:56:50.788659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- At this point, the text data are clean of any odd whitespace and non-English characters\n- I could clean the text data further, such as removing numbers, punctuation, etc., but I will do that in a separate module focused on processing English text with specialized tools","metadata":{}},{"cell_type":"markdown","source":"## Data reduction - feature selection\n\n- Now I will drop any columns I no longer need:\n    - `text` - the raw, un-cleaned text\n- Double check the data in the remaining columns look as expected","metadata":{}},{"cell_type":"code","source":"# We can now drop the unprocessed text column\ndf = df.drop(columns=['text'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:50.790889Z","iopub.execute_input":"2024-05-30T21:56:50.791235Z","iopub.status.idle":"2024-05-30T21:56:50.806172Z","shell.execute_reply.started":"2024-05-30T21:56:50.791209Z","shell.execute_reply":"2024-05-30T21:56:50.804848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Save data\n___\n\n- I can finally save the reduced and cleaned data as an output file for use in other modules\n- I am saving the data in the JSON format to remain consistent with the input files","metadata":{}},{"cell_type":"code","source":"# The data are now prepared!\n# The next step will be to process the text data and featurize the text and class labels\n\n# Save the current state of the data so it can be read by other notebooks\ndf.to_json('cleaned_reduced_data.json', orient='records', lines=True)\n\n# Read the saved data back in to verify the format\ndf = pd.read_json('/kaggle/working/cleaned_reduced_data.json', orient='records', lines=True)\n\n# Inspect the size and data-types of the df\ndf.info()\n\n# Inspect the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-30T21:56:50.807338Z","iopub.execute_input":"2024-05-30T21:56:50.807633Z","iopub.status.idle":"2024-05-30T21:56:51.109762Z","shell.execute_reply.started":"2024-05-30T21:56:50.807609Z","shell.execute_reply":"2024-05-30T21:56:51.108585Z"},"trusted":true},"execution_count":null,"outputs":[]}]}