{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":8960525,"datasetId":5117206,"databundleVersionId":9124217}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Module 2 - NLP*\n___","metadata":{}},{"cell_type":"markdown","source":"# 1. Project summary\n___\n\nThis is part of a project to build a sentiment classifier trained on Yelp review data (https://www.yelp.com/dataset). The project has been divided into several modules to perform different parts of the analysis, e.g., data cleaning, data processing, and model training. The goal is to predict the sentiment of a document; while using Yelp reviews of businesses, the 1-5 star rating acts as a proxy for sentiment, and the written Yelp review as the document text. The project is written in Python on Jupyter notebooks and makes use of a range of data science tools like pandas, spaCy, word2vec, and keras. My motivation in starting this project is to build my skillset, learn new tools, and improve as a data scientist. It is an ongoing project and may see many updates/iterations.","metadata":{}},{"cell_type":"markdown","source":"# 2. Module overview\n___\n\n## Goal\n- The goal of this module is to further prepare the Yelp review data by performing natural language processing on the text data\n    - Tokenization:\n        - Text data are passed through a pretrained English language model that will tokenize the text\n    - Filtering:\n        - Unnecssesary tokens like punctuation will be filtered from the text\n    - Lemmatization:\n        - Now filtered tokens will be lemmatized for sentiment analysis\n- The above steps are organized for clarity but are not performed in the exact order listed\n- Processed data are saved in an ouput json file for processing by the next module\n\n## Data\n- Input Data:\n    - `yelp-dataset-reviews-prepared`\n    - Kaggle: https://www.kaggle.com/datasets/gabrielmadigan/yelp-dataset-reviews-prepared/data/intermediate\n    - Derived from: `yelp-dataset`\n        - Kaggle: https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset\n        - Yelp: https://www.yelp.com/dataset\n        - Description from the kaggle dataset page:\n        > This dataset is a subset of Yelp's businesses, reviews, and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries. In the most recent dataset you'll find information about businesses across 8 metropolitan areas in the USA and Canada.\n- This dataset has been prepared for this module following inspection, cleaning, and reduction of the original Yelp dataset.\n\n## Libraries\n- Key Libraries:\n    - `Pandas` - used to read, load, store, inspect, process, and save the data\n        - webpage: https://pandas.pydata.org/\n    - `spaCy` - a powerful natural language processing library used for tokenizing the text data\n        - webpage: https://spacy.io/\n\n## Output\n- Output:\n    - `/kaggle/working/data/intermediate/lemmatized-data.json` - contains the lemmatized reviews and corresponding labels\n    - `/kaggle/working/data/intermediate/lemmatized-sentences.json` - contains lemmatized sentences from all reviews, needed when we train the word embeddings in the next module","metadata":{}},{"cell_type":"markdown","source":"# 3. Import libraries\n___\n\n- I will be importing `pandas` to read, load, and handle the data\n- The `spaCy` library is used to perform NLP on the text data\n- `spaCy` comes with pretrained language models that can be loaded and used to process text, including: tokenization, parts of speech tagging, lemmatization, named entity recognition, etc.,\n- The `Sentencizer` module from `spaCy` is used to extract individual sentences from a document that we will need for word embeddings in a later module\n- The `tqdm` package is useful for displaying progress bars while processing data (https://tqdm.github.io/)\n- The module from `IPython` ensures every command in a cell is displayed, which saves me from having to write lots of print statements.","metadata":{}},{"cell_type":"code","source":"# Libraries for reading and handling data\nimport os\nimport pandas as pd\n\n# Libraries for NLP\nimport spacy\nfrom spacy.tokens import Doc\nfrom spacy.pipeline import Sentencizer\n\n# tqdm allows us to display a progress bar for long loops\nfrom tqdm import tqdm \n\n# Settings for displaying commands in a cell\nfrom IPython.core.interactiveshell import InteractiveShell","metadata":{"_uuid":"3ce37478-79fc-4303-ac12-b61b4055bef4","_cell_guid":"4340e0b8-63aa-463b-9c4f-5c6c3e3a4b6d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-15T16:35:20.826044Z","iopub.execute_input":"2024-07-15T16:35:20.826444Z","iopub.status.idle":"2024-07-15T16:35:27.874504Z","shell.execute_reply.started":"2024-07-15T16:35:20.826392Z","shell.execute_reply":"2024-07-15T16:35:27.873379Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"- Some settings for the notebook that aid with analysis","metadata":{}},{"cell_type":"code","source":"# Display output of every command in a cell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# shows a progress bar while manipulating a pandas df or series\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:35:27.876595Z","iopub.execute_input":"2024-07-15T16:35:27.877261Z","iopub.status.idle":"2024-07-15T16:35:27.883593Z","shell.execute_reply.started":"2024-07-15T16:35:27.877221Z","shell.execute_reply":"2024-07-15T16:35:27.882284Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 4. Read data\n___\n\n- Read the input files and load the data into a pandas dataframe\n- Inspect the dataframe for data-type, size, contents, etc.,","metadata":{}},{"cell_type":"code","source":"# Read in the cleaned/prepared Yelp review data as a pandas dataframe\nINPUT_FILE = '/kaggle/input/yelp-dataset-reviews-prepared/data/intermediate/cleaned-reduced-data.json'\nOUT_PATH = '/kaggle/working/'\n\n# Save all intermediate data to this directory\nDATA_PATH = os.path.join(OUT_PATH, 'data')\nif not os.path.exists(DATA_PATH):\n    os.mkdir(DATA_PATH)\nINT_DATA_PATH = os.path.join(DATA_PATH, 'intermediate')\nif not os.path.exists(INT_DATA_PATH):\n    os.mkdir(INT_DATA_PATH)\n\ndf = pd.read_json(INPUT_FILE, lines=True)\n\n# Inspect the size and data-types of the df\ndf.info()\n\n# Inspect the first few rows\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:35:27.885384Z","iopub.execute_input":"2024-07-15T16:35:27.885843Z","iopub.status.idle":"2024-07-15T16:35:28.233728Z","shell.execute_reply.started":"2024-07-15T16:35:27.885804Z","shell.execute_reply":"2024-07-15T16:35:28.232356Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   stars         20000 non-null  int64 \n 1   cleaned_text  20000 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 312.6+ KB\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   stars                                       cleaned_text\n0      3  If you decide to eat here, just be aware it is...\n1      5  I've taken a lot of spin classes over the year...\n2      3  Family diner. Had the buffet. Eclectic assortm...\n3      5  Wow! Yummy, different, delicious. Our favorite...\n4      4  Cute interior and owner (?) gave us tour of up...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Wow! Yummy, different, delicious. Our favorite...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 5. Data featurization - NLP\n___\n\n## Load English language pipeline with spaCy\n\n- To begin processing the text data, I construct a spaCy pipeline by loading in the pretrained English language model: `en_core_web_lg`\n- Then I add specific tools to the pipeline:\n    - `sentencizer` for identifying sentences (needed for training the word embeddings later on)\n    - `merge_entities` for identifying names and merging them into a single token (we want to treat, e.g., a restaurant's name as a single word)","metadata":{}},{"cell_type":"code","source":"# load English language pipeline from spacy\n# Add sentencizer; we will need sentences for word2vec training\n# Add merge_entities to boost performance; e.g., treat \"Starbucks Coffee\" as one token, not two, i.e., \"Starbucks\" and \"Coffee\"\nnlp = spacy.load('en_core_web_lg')\nnlp.add_pipe('sentencizer')\nnlp.add_pipe('merge_entities')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:35:28.236768Z","iopub.execute_input":"2024-07-15T16:35:28.237255Z","iopub.status.idle":"2024-07-15T16:35:33.036697Z","shell.execute_reply.started":"2024-07-15T16:35:28.237213Z","shell.execute_reply":"2024-07-15T16:35:33.035582Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<spacy.pipeline.sentencizer.Sentencizer at 0x7996919ba940>"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<function spacy.pipeline.functions.merge_entities(doc: spacy.tokens.doc.Doc)>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenize text\n\n- I can now pass text as a string to the pretrained pipeline that will return a spaCy `Doc` object\n- A spaCy `Doc` is a container for all the information gathered in the pipeline, including the original text, word tokens, annotations, etc.,\n- Tokenize the text data by creating a `Doc` for each Yelp review, and add it to the dataframe in a new column","metadata":{}},{"cell_type":"code","source":"# Tokenize the cleaned text\n# Pass the text into the language pipe and return a list spacy docs \n# NB: this can take a long time - passing the df to a pipe speeds processing time vs. passing the language model to each row in the df with the apply method\ndf['doc'] = list(tqdm(nlp.pipe(df['cleaned_text'], batch_size=64, n_process=4), total=len(df.index)))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:35:33.038154Z","iopub.execute_input":"2024-07-15T16:35:33.038601Z","iopub.status.idle":"2024-07-15T16:38:14.499701Z","shell.execute_reply.started":"2024-07-15T16:35:33.038564Z","shell.execute_reply":"2024-07-15T16:38:14.498516Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [02:41<00:00, 123.91it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- spaCy comes with a great visualization tool called `displacy`\n- Let's use this to look at the performance of the named entity recognition","metadata":{}},{"cell_type":"code","source":"# Display an example doc with named entities highlighted to verify performance of spacy pipe\nspacy.displacy.render(df['doc'][1], style='ent', jupyter=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:14.501284Z","iopub.execute_input":"2024-07-15T16:38:14.501664Z","iopub.status.idle":"2024-07-15T16:38:14.512170Z","shell.execute_reply.started":"2024-07-15T16:38:14.501632Z","shell.execute_reply":"2024-07-15T16:38:14.511021Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I've taken a lot of spin classes over \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    the years\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n, and nothing compares to the classes at \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Body Cycle\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n. From the nice, clean space and amazing bikes, to the welcoming and motivating instructors, every class is a top notch work out. For anyone who struggles to fit workouts in, the online scheduling system makes it easy to plan ahead (and there's no need to line up way in advanced like many gyms make you do). There is no way I can write this review without giving \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Russell\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n, the owner of \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Body Cycle\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n, a shout out. \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Russell\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n's passion for fitness and cycling is so evident, as is his desire for all of his clients to succeed. He is always dropping in to classes to check in/provide encouragement, and is open to ideas and recommendations from anyone. \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Russell\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n always wears a smile on his face, even when he's kicking your butt in class!</div></span>"},"metadata":{}}]},{"cell_type":"markdown","source":"- Not too bad; the model correctly identified:\n    - \"`Body Cycle`\" as an `ORG` in both instances - also it merged the two words \"`Body`\" and \"`Cycle`\" into a single token\n    - \"`Russell`\" as a `PERSON` in all 3 instances\n    - \"`the years`\" as a `DATE` - not super critical for this analysis but validates the overall performance","metadata":{}},{"cell_type":"markdown","source":"## Lemmatize and filter text\n\n- For sentiment analysis, we do not need (or want) high variation in our text, so lemmatizing the text should yield a better model\n- The lemmatization of a word is saved as a variable in each token\n- Define two functions that loop through the tokens of a Doc and get a list of the lemmmas as strings\n- For the first funtion, also apply filtering to the text, removing stop words and punctuation that can have adverse effects on sentiment classifier training (as with lemmatiziation)\n- The second funtion will process the text for training the word embeddings later on, which will benifit from a higher density of contextual information like punctuation and stop words - apply no filters","metadata":{}},{"cell_type":"code","source":"# Some functions for processing tokenized text\ndef lemmatize_filter(doc: Doc) -> list[str]:\n    '''Lemmatize tokens and apply filters to doc.'''\n    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n\ndef lemmatize(doc: Doc) -> list[str]:\n    '''Lemmatize tokens in doc.'''\n    return [token.lemma_ for token in doc]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:14.513914Z","iopub.execute_input":"2024-07-15T16:38:14.514366Z","iopub.status.idle":"2024-07-15T16:38:14.523734Z","shell.execute_reply.started":"2024-07-15T16:38:14.514327Z","shell.execute_reply":"2024-07-15T16:38:14.522376Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"- Apply the lemmatization and filtering to each review Doc and add to the dataframe as a new column","metadata":{}},{"cell_type":"code","source":"# Lematize the docs and add the list of lemmas to the df\ndf['lemmatized_text'] = df['doc'].progress_apply(lemmatize_filter)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:14.525056Z","iopub.execute_input":"2024-07-15T16:38:14.525385Z","iopub.status.idle":"2024-07-15T16:38:15.755558Z","shell.execute_reply.started":"2024-07-15T16:38:14.525350Z","shell.execute_reply":"2024-07-15T16:38:15.754269Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 20000/20000 [00:01<00:00, 16819.18it/s]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   stars                                       cleaned_text  \\\n0      3  If you decide to eat here, just be aware it is...   \n1      5  I've taken a lot of spin classes over the year...   \n2      3  Family diner. Had the buffet. Eclectic assortm...   \n3      5  Wow! Yummy, different, delicious. Our favorite...   \n4      4  Cute interior and owner (?) gave us tour of up...   \n\n                                                 doc  \\\n0  (If, you, decide, to, eat, here, ,, just, be, ...   \n1  (I, 've, taken, a, lot, of, spin, classes, ove...   \n2  (Family, diner, ., Had, the, buffet, ., Eclect...   \n3  (Wow, !, Yummy, ,, different, ,, delicious, .,...   \n4  (Cute, interior, and, owner, (, ?, ), gave, us...   \n\n                                     lemmatized_text  \n0  [decide, eat, aware, go, about 2 hour, begin, ...  \n1  [take, lot, spin, class, the year, compare, cl...  \n2  [family, diner, buffet, eclectic, assortment, ...  \n3  [wow, yummy, different, delicious, favorite, l...  \n4  [cute, interior, owner, give, tour, upcoming, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>cleaned_text</th>\n      <th>doc</th>\n      <th>lemmatized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>If you decide to eat here, just be aware it is...</td>\n      <td>(If, you, decide, to, eat, here, ,, just, be, ...</td>\n      <td>[decide, eat, aware, go, about 2 hour, begin, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>I've taken a lot of spin classes over the year...</td>\n      <td>(I, 've, taken, a, lot, of, spin, classes, ove...</td>\n      <td>[take, lot, spin, class, the year, compare, cl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n      <td>(Family, diner, ., Had, the, buffet, ., Eclect...</td>\n      <td>[family, diner, buffet, eclectic, assortment, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>Wow! Yummy, different, delicious. Our favorite...</td>\n      <td>(Wow, !, Yummy, ,, different, ,, delicious, .,...</td>\n      <td>[wow, yummy, different, delicious, favorite, l...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Cute interior and owner (?) gave us tour of up...</td>\n      <td>(Cute, interior, and, owner, (, ?, ), gave, us...</td>\n      <td>[cute, interior, owner, give, tour, upcoming, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Separate and lemmatize sentences\n- Separate out each sentence in each review Doc and store the sentences in new dataframe \n- Apply the lemmatization with NO filtering to each sentence in the new dataframe","metadata":{}},{"cell_type":"code","source":"# Make new df containing all sentences in all docs\n# This will be saved and used when creating word embeddings in the future\n# Lemmatize but do not filter - we want as much context as possible to create meaningful word embeddings\nsents = []\nfor i, doc in enumerate(df['doc']):\n    for sent in doc.sents:\n        sents.append(sent)\nsents_df = pd.DataFrame({'sents': sents})\nsents_df['lemmatized_sents'] = sents_df['sents'].progress_apply(lemmatize)\nsents_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:15.757521Z","iopub.execute_input":"2024-07-15T16:38:15.757950Z","iopub.status.idle":"2024-07-15T16:38:18.642822Z","shell.execute_reply.started":"2024-07-15T16:38:15.757914Z","shell.execute_reply":"2024-07-15T16:38:18.641577Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 155488/155488 [00:02<00:00, 76659.71it/s]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               sents  \\\n0  (If, you, decide, to, eat, here, ,, just, be, ...   \n1  (We, have, tried, it, multiple, times, ,, beca...   \n2  (I, have, been, to, it, 's, other, locations, ...   \n3  (The, food, is, good, ,, but, it, takes, a, ve...   \n4  (The, waitstaff, is, very, young, ,, but, usua...   \n\n                                    lemmatized_sents  \n0  [if, you, decide, to, eat, here, ,, just, be, ...  \n1  [we, have, try, it, multiple, time, ,, because...  \n2  [I, have, be, to, it, be, other, location, in,...  \n3  [the, food, be, good, ,, but, it, take, a, ver...  \n4  [the, waitstaff, be, very, young, ,, but, usua...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sents</th>\n      <th>lemmatized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(If, you, decide, to, eat, here, ,, just, be, ...</td>\n      <td>[if, you, decide, to, eat, here, ,, just, be, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(We, have, tried, it, multiple, times, ,, beca...</td>\n      <td>[we, have, try, it, multiple, time, ,, because...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(I, have, been, to, it, 's, other, locations, ...</td>\n      <td>[I, have, be, to, it, be, other, location, in,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(The, food, is, good, ,, but, it, takes, a, ve...</td>\n      <td>[the, food, be, good, ,, but, it, take, a, ver...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(The, waitstaff, is, very, young, ,, but, usua...</td>\n      <td>[the, waitstaff, be, very, young, ,, but, usua...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 6. Data reduction - feature removal\n___\n\n- The original review text and review Docs are now irrelevant features, so we can remove these columns from the \"reviews\" dataframe\n- For the \"sentences\" dataframe, we no longer need the un-lemmatized sentence Docs, so we drop that column as well","metadata":{}},{"cell_type":"code","source":"# We can now drop the cleaned_text and doc columns\ndf = df.drop(columns=['cleaned_text', 'doc'])\ndf.head()\nsents_df = sents_df.drop(columns=['sents'])\nsents_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:18.645873Z","iopub.execute_input":"2024-07-15T16:38:18.646231Z","iopub.status.idle":"2024-07-15T16:38:18.678472Z","shell.execute_reply.started":"2024-07-15T16:38:18.646200Z","shell.execute_reply":"2024-07-15T16:38:18.677368Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   stars                                    lemmatized_text\n0      3  [decide, eat, aware, go, about 2 hour, begin, ...\n1      5  [take, lot, spin, class, the year, compare, cl...\n2      3  [family, diner, buffet, eclectic, assortment, ...\n3      5  [wow, yummy, different, delicious, favorite, l...\n4      4  [cute, interior, owner, give, tour, upcoming, ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>lemmatized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>[decide, eat, aware, go, about 2 hour, begin, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>[take, lot, spin, class, the year, compare, cl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[family, diner, buffet, eclectic, assortment, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>[wow, yummy, different, delicious, favorite, l...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[cute, interior, owner, give, tour, upcoming, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                    lemmatized_sents\n0  [if, you, decide, to, eat, here, ,, just, be, ...\n1  [we, have, try, it, multiple, time, ,, because...\n2  [I, have, be, to, it, be, other, location, in,...\n3  [the, food, be, good, ,, but, it, take, a, ver...\n4  [the, waitstaff, be, very, young, ,, but, usua...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[if, you, decide, to, eat, here, ,, just, be, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[we, have, try, it, multiple, time, ,, because...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[I, have, be, to, it, be, other, location, in,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[the, food, be, good, ,, but, it, take, a, ver...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[the, waitstaff, be, very, young, ,, but, usua...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 7. Save data\n___\n- I can finally save the processed data and sentences as output files for use in other modules\n- I am saving the data in the JSON format to remain consistent with the input files","metadata":{}},{"cell_type":"code","source":"# The reviews are now processed into sequences of lemmas\n# Also every sentence in the reviews is now processed into a sequence of lemmas\n# The next step will be to further process the text data into word embeddings\n\n# Save the current state of the data so it can be read by other notebooks\ndf.to_json(os.path.join(INT_DATA_PATH, 'lemmatized-data.json'), orient='records', lines=True)\nsents_df.to_json(os.path.join(INT_DATA_PATH, 'lemmatized-sentences.json'), orient='records', lines=True)\n\n# Read the saved data back in to verify the format\ndf = pd.read_json(os.path.join(INT_DATA_PATH, 'lemmatized-data.json'), orient='records', lines=True)\nsents_df = pd.read_json(os.path.join(INT_DATA_PATH, 'lemmatized-sentences.json'), orient='records', lines=True)\n\n# Inspect the size and data-types of the df\ndf.info()\nsents_df.info()\n\n# Inspect the first few rows\ndf.head()\nsents_df.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T16:38:18.680030Z","iopub.execute_input":"2024-07-15T16:38:18.680399Z","iopub.status.idle":"2024-07-15T16:38:20.932326Z","shell.execute_reply.started":"2024-07-15T16:38:18.680367Z","shell.execute_reply":"2024-07-15T16:38:20.931135Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20000 entries, 0 to 19999\nData columns (total 2 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   stars            20000 non-null  int64 \n 1   lemmatized_text  20000 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 312.6+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 155488 entries, 0 to 155487\nData columns (total 1 columns):\n #   Column            Non-Null Count   Dtype \n---  ------            --------------   ----- \n 0   lemmatized_sents  155488 non-null  object\ndtypes: object(1)\nmemory usage: 1.2+ MB\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   stars                                    lemmatized_text\n0      3  [decide, eat, aware, go, about 2 hour, begin, ...\n1      5  [take, lot, spin, class, the year, compare, cl...\n2      3  [family, diner, buffet, eclectic, assortment, ...\n3      5  [wow, yummy, different, delicious, favorite, l...\n4      4  [cute, interior, owner, give, tour, upcoming, ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>lemmatized_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>[decide, eat, aware, go, about 2 hour, begin, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>[take, lot, spin, class, the year, compare, cl...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[family, diner, buffet, eclectic, assortment, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>[wow, yummy, different, delicious, favorite, l...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>[cute, interior, owner, give, tour, upcoming, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                    lemmatized_sents\n0  [if, you, decide, to, eat, here, ,, just, be, ...\n1  [we, have, try, it, multiple, time, ,, because...\n2  [I, have, be, to, it, be, other, location, in,...\n3  [the, food, be, good, ,, but, it, take, a, ver...\n4  [the, waitstaff, be, very, young, ,, but, usua...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmatized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[if, you, decide, to, eat, here, ,, just, be, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[we, have, try, it, multiple, time, ,, because...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[I, have, be, to, it, be, other, location, in,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[the, food, be, good, ,, but, it, take, a, ver...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[the, waitstaff, be, very, young, ,, but, usua...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}